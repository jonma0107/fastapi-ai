# API IA - FastAPI + Ollama

## üöÄ Inicio r√°pido con Docker

Este proyecto utiliza **Docker** para facilitar la ejecuci√≥n y despliegue de una API basada en FastAPI que interact√∫a con modelos LLM de **Ollama**.

- El archivo `Dockerfile` define el entorno Python, instala dependencias con Poetry y expone el puerto 8000 para la API.
- El archivo `docker-compose.yml` permite levantar el servicio f√°cilmente, usando el modo de red `host` para que el contenedor pueda comunicarse con Ollama corriendo en tu m√°quina.

### Pasos para levantar la API

1. **Aseg√∫rate de tener Docker y Docker Compose instalados.**
2. **Aseg√∫rate de que Ollama est√© corriendo en tu host** (por ejemplo, con `ollama serve`).
3. **Levanta la API:**
   ```bash
   docker compose up --build
   ```
4. Accede a la documentaci√≥n interactiva en: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üß† ¬øQu√© hace la API? (`src/ollama_app.py`)

Esta API expone un endpoint inteligente que permite enviar una **pregunta** de dos formas:
- Solo texto (por ejemplo: "¬øCu√°l es la capital de Francia?")
- Texto acompa√±ado de una **imagen** (por ejemplo: "¬øQu√© contiene esta imagen?" + archivo)

El backend procesa la imagen (si se env√≠a), consulta un modelo LLM de Ollama y devuelve una respuesta enriquecida y estructurada.

### **L√≥gica principal:**

- **Modelos de datos:**  
  Se definen modelos Pydantic para estructurar tanto la pregunta como la respuesta. La respuesta incluye:
  - La respuesta directa (`answer`)
  - El razonamiento del modelo (`thoughts`)
  - El tema identificado (`topic`)

- **Procesamiento de la imagen:**  
  Si se env√≠a una imagen, se convierte autom√°ticamente a base64 para ser entendida por el modelo LLM. Si no se env√≠a imagen, el modelo responde solo en base a la pregunta de texto.

- **Interacci√≥n con Ollama:**  
  Se consulta un modelo LLM de Ollama (por defecto, `gemma3:latest`), pidi√©ndole que responda **siempre en formato JSON** con los campos esperados.

- **Validaci√≥n y logging:**  
  La respuesta del modelo se valida y estructura antes de devolverse al usuario. Adem√°s, cada interacci√≥n se registra en un archivo de log para auditor√≠a y an√°lisis.

- **Endpoint principal:**  
  - **POST `/api/question`**  
    Recibe:
    - `question`: Texto de la pregunta (campo de formulario, obligatorio)
    - `file`: Imagen a analizar (archivo, opcional)
    Devuelve:
    - Respuesta estructurada con la respuesta, razonamiento y tema.

---

## üìÑ Ejemplo de uso en Swagger

1. Ve a [http://localhost:8000/docs](http://localhost:8000/docs)
2. Usa el endpoint `/api/question`
3. Escribe tu pregunta y, si lo deseas, sube una imagen.
4. Obtendr√°s una respuesta enriquecida y explicada por el modelo.

---

## üìù Notas

- Es necesario que el servidor Ollama est√© corriendo y accesible desde el contenedor Docker.
- El endpoint permite preguntas de solo texto o preguntas con imagen; la imagen es opcional. 